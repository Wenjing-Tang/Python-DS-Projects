{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionaries ##\n",
    "\n",
    "# Definition of countries and capital\n",
    "countries = ['spain', 'france', 'germany', 'norway']\n",
    "capitals = ['madrid', 'paris', 'berlin', 'oslo']\n",
    "\n",
    "# Get index of 'germany': ind_ger\n",
    "ind_ger = countries.index(\"germany\")\n",
    "\n",
    "# Use ind_ger to print out capital of Germany\n",
    "print(capitals[ind_ger])\n",
    "\n",
    "# From string in countries and capitals, create dictionary europe\n",
    "europe = { 'spain':'madrid', 'france': 'paris', 'germany':'berlin', 'norway': 'oslo' }\n",
    "\n",
    "# Add italy to europe\n",
    "europe['italy'] = 'rome'\n",
    "\n",
    "# Print out italy in europe\n",
    "print('italy' in europe)\n",
    "\n",
    "# Definition of dictionary\n",
    "europe = {'spain':'madrid', 'france':'paris', 'germany':'bonn',\n",
    "          'norway':'oslo', 'italy':'rome', 'poland':'warsaw',\n",
    "          'australia':'vienna' }\n",
    "\n",
    "# Update capital of germany\n",
    "europe['germany'] = 'berlin'\n",
    "\n",
    "# Remove australia\n",
    "del(europe['australia'])\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## DataFrame ## \n",
    "\n",
    "# Pre-defined lists\n",
    "names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\n",
    "dr =  [True, False, False, False, True, True, True]\n",
    "cpc = [809, 731, 588, 18, 200, 70, 45]\n",
    "\n",
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Create dictionary my_dict with three key:value pairs: my_dict\n",
    "my_dict = {\"country\": names, 'drives_right': dr, 'cars_per_cap' : cpc}\n",
    "\n",
    "# Build a DataFrame cars from my_dict: cars\n",
    "cars = pd.DataFrame(my_dict)\n",
    "\n",
    "# Print cars\n",
    "print(cars)\n",
    "\n",
    "# Build cars DataFrame\n",
    "names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\n",
    "dr =  [True, False, False, False, True, True, True]\n",
    "cpc = [809, 731, 588, 18, 200, 70, 45]\n",
    "cars_dict = { 'country':names, 'drives_right':dr, 'cars_per_cap':cpc }\n",
    "cars = pd.DataFrame(cars_dict)\n",
    "print(cars)\n",
    "\n",
    "# Definition of row_labels\n",
    "row_labels = ['US', 'AUS', 'JPN', 'IN', 'RU', 'MOR', 'EG']\n",
    "\n",
    "# Specify row labels of cars\n",
    "cars.index = row_labels\n",
    "\n",
    "# Print cars again\n",
    "print(cars)\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## loc and iloc ##\n",
    "\n",
    "# Import cars data\n",
    "import pandas as pd\n",
    "cars = pd.read_csv('cars.csv', index_col = 0)\n",
    "\n",
    "# Print out observation for Japan\n",
    "print(cars.loc['JPN'])\n",
    "print(cars.iloc[2])\n",
    "\n",
    "# Print out observations for Australia and Egypt\n",
    "print(cars.loc[['AUS', 'EG']])\n",
    "print(cars.iloc[[1, 6]])\n",
    "\n",
    "# Print out drives_right value of Morocco\n",
    "print(cars.loc[['MOR'], ['drives_right']])\n",
    "\n",
    "# Print sub-DataFrame\n",
    "print(cars.loc[['RU', 'MOR'], ['country', 'drives_right']])\n",
    "\n",
    "# Print out drives_right column as Series\n",
    "print(cars.loc[:, 'drives_right'])\n",
    "\n",
    "# Print out drives_right column as DataFrame\n",
    "print(cars.loc[:, ['drives_right']])\n",
    "\n",
    "# Print out cars_per_cap and drives_right as DataFrame\n",
    "print(cars.loc[:, ['cars_per_cap', 'drives_right']])\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## Random Walk ## \n",
    "\n",
    "# Initialize random_walk\n",
    "random_walk = [0]\n",
    "\n",
    "# Complete the ___\n",
    "for x in range(100) :\n",
    "    # Set step: last element in random_walk\n",
    "    step = random_walk[-1]\n",
    "\n",
    "    # Roll the dice\n",
    "    dice = np.random.randint(1,7)\n",
    "\n",
    "    # Determine next step\n",
    "    if dice <= 2:\n",
    "        step = step - 1\n",
    "    elif dice <= 5:\n",
    "        step = step + 1\n",
    "    else:\n",
    "        step = step + np.random.randint(1,7)\n",
    "\n",
    "    # append next_step to random_walk\n",
    "    random_walk.append(step)\n",
    "\n",
    "# Print random_walk\n",
    "print(random_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by ## \n",
    "\n",
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type/sales_by_type.sum()\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## Pivot ##\n",
    "\n",
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index =\"type\")\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## Find Missing Values ##\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind = \"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Remove missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "print(avocados_complete.isna().any())\n",
    "\n",
    "# Replace missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging DataFrame with Pandas ##\n",
    "\n",
    "# Reading DataFrames from multiple files in a loop\n",
    "import pandas as pd\n",
    "filenames = ['Gold.csv', 'Silver.csv', 'Bronze.csv']\n",
    "\n",
    "dataframes = []\n",
    "for filename in filenames:\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "print(dataframes[0].head())\n",
    "\n",
    "# Reading multiple files to build a DataFrame\n",
    "medals =[]\n",
    "\n",
    "for medal in medal_types:\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    columns = ['Country', medal]\n",
    "    medal_df = pd.read_csv(file_name, header = 0, index_col= \"Country\", names = columns)\n",
    "    medals.append(medal_df)\n",
    "\n",
    "medals_df = pd.concat(medals, axis = \"columns\")\n",
    "print(medals_df)\n",
    "\n",
    "# Combining DataFrames from multiple data files\n",
    "medals = gold.copy()\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "medals.columns = new_labels\n",
    "\n",
    "medals['Silver'] = silver[\"Total\"]\n",
    "medals['Bronze'] = bronze[\"Total\"]\n",
    "\n",
    "print(medals.head())\n",
    "\n",
    "# Sorting DataFrame with the Index & columns\n",
    "weather1 = pd.read_csv(\"monthly_max_temp.csv\", index_col = \"Month\")\n",
    "weather2 = weather1.sort_index()\n",
    "weather3 = weather1.sort_index(ascending = False)\n",
    "weather4 = weather1.sort_values(\"Max TemperatureF\")\n",
    "\n",
    "print(weather4.head())\n",
    "\n",
    "# Concatenating pandas DataFrames along column axis\n",
    "weather_list = [weather_max, weather_mean]\n",
    "weather = pd.concat(weather_list, axis = 1)\n",
    "\n",
    "print(weather)\n",
    "\n",
    "# Concatenating pandas DataFrames along row axis\n",
    "weather_list = [weather_max, weather_mean]\n",
    "weather = pd.concat(weather_list, axis = 0)\n",
    "\n",
    "print(weather)\n",
    "\n",
    "# Concatenating DataFrames from a dict\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "    month_dict[month_name] = month_data.groupby(\"Company\").sum()\n",
    "\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])\n",
    "\n",
    "# Merging on a specific column\n",
    "merge_by_city = pd.merge(revenue, managers, on = \"city\")\n",
    "combined = pd.merge(revenue, managers, left_on=\"city\", right_on=\"branch\")\n",
    "combined2 = pd.merge(revenue, managers, on = [\"branch_id\", \"city\", \"state\"])\n",
    "sales_and_managers = pd.merge(sales, managers, left_on = [\"city\", \"state\"], right_on=[\"branch\", \"state\"], how = \"left\")\n",
    "\n",
    "# Perform the third ordered merge: tx_weather_ffill\n",
    "tx_weather_ffill = pd.merge_ordered(austin, houston, on = \"date\", suffixes=['_aus','_hus'], fill_method=\"ffill\")\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## Case Study ##\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'\n",
    "editions = pd.read_csv(file_path, sep='\\t')\n",
    "editions = editions[['Edition', 'Grand Total', 'City', 'Country']]\n",
    "\n",
    "ioc_codes = pd.read_csv(file_path)\n",
    "ioc_codes = ioc_codes[[\"Country\", \"NOC\"]]\n",
    "\n",
    "# Create empty dictionary: medals_dict\n",
    "medals_dict = {}\n",
    "\n",
    "for year in editions['Edition']:\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    medals_dict[year] = pd.read_csv(file_path)\n",
    "    medals_dict[year] = medals_dict[year][['Athlete','NOC', 'Medal']]\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict, ignore_index=True)\n",
    "\n",
    "# Construct the pivot_table: medal_counts\n",
    "medal_counts = medals.pivot_table(index = \"Edition\", values=\"Athlete\", columns = \"NOC\", aggfunc=\"count\")\n",
    "\n",
    "# Set Index of editions: totals\n",
    "totals = editions.set_index(\"Edition\")\n",
    "totals = totals['Grand Total']\n",
    "fractions = medal_counts.divide(totals, axis = \"rows\")\n",
    "\n",
    "# Left join editions and ioc_codes: hosts\n",
    "hosts = pd.merge(editions, ioc_codes, how = \"left\", on = \"Country\")\n",
    "\n",
    "# Extract relevant columns and set index: hosts\n",
    "hosts = hosts[[\"Edition\", \"NOC\"]].set_index(\"Edition\")\n",
    "\n",
    "# Fix missing 'NOC' values of hosts\n",
    "print(hosts.loc[hosts.NOC.isnull()])\n",
    "hosts.loc[1972, 'NOC'] = 'FRG'\n",
    "hosts.loc[1980, 'NOC'] = 'URS'\n",
    "hosts.loc[1988, 'NOC'] = 'KOR'\n",
    "hosts = hosts.reset_index()\n",
    "\n",
    "# Reshape fractions_change: reshaped\n",
    "reshaped = pd.melt(fractions_change, id_vars='Edition', value_name='Change')\n",
    "print(reshaped.shape, fractions_change.shape)\n",
    "chn = reshaped[reshaped[\"NOC\"] == 'CHN']\n",
    "\n",
    "# Merge reshaped and hosts: merged\n",
    "merged = pd.merge(reshaped, hosts, how = \"inner\")\n",
    "influence = merged.set_index(\"Edition\").sort_index()\n",
    "\n",
    "print(influence.head())\n",
    "\n",
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "change = influence[\"Change\"]\n",
    "ax = change.plot(kind =\"bar\")\n",
    "\n",
    "# Customize the plot to improve readability\n",
    "ax.set_ylabel(\"% Change of Host Country Medal Count\")\n",
    "ax.set_title(\"Is there a Host Country Advantage?\")\n",
    "ax.set_xticklabels(editions['City'])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intermediate Importing Data in Python ## \n",
    "\n",
    "# Reading a text file.1: \n",
    "filename = 'huck_finn.txt'\n",
    "file = open(filename, mode = 'r')        (read-only)\n",
    "text = file.read()\n",
    "file.close()\n",
    "print(text)\n",
    "\n",
    "# Reading a text file.2: \n",
    "Context Manager: with open('huck_finn.txt', r) as file:\n",
    "print(file.read( ))\n",
    "\n",
    "# Using NumPy importing flat files: (Numerical)\n",
    "filename = 'huck_finn.txt'\n",
    "data = np.loadtxt(filename, delimiter = ',', skiprows = 1, usecols = [0, 2]) \n",
    "(',' for comma-delimited,  '\\t' for tab-delimited)\n",
    "(usecols = [0, 2]: return the first and third columns)\n",
    "\n",
    "# Using Pandas importing flat files: (String)\n",
    "filename = 'huck_finn.txt'\n",
    "data = pd.read_csv(filename, sep='\\t', comment='#', na_values='Nothing')\n",
    "(contains comments after the character '#')\n",
    "------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Pickled files: \n",
    "    import pickle \n",
    "            with open('huck_finn.pkl', 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "    print(data)\n",
    "\n",
    "# Excel:\n",
    "    import pandas as pd\n",
    "           file = 'huck_finn.xlsx'\n",
    "           data = pd.ExcelFile(file)\n",
    "    print(data.sheet_names)\n",
    " \n",
    "\n",
    "# SAS: \n",
    "    import pandas as pd\n",
    "    from sas7bdat import SAS7BDAT:\n",
    "    with SAS7BDAT('huck_finn.sas7bdat') as file:\n",
    "        df_sas = file.to_data_frame()\n",
    "\n",
    "# STATA: \n",
    "    data = pd.read_stata('huck_finn.dta')\n",
    "\n",
    "# HDF5: \n",
    "    import h5py\n",
    "        filename = 'huck_finn.hdf5'\n",
    "        data = h5py.File(filename, 'r')\n",
    "\n",
    "# MATLAB: \n",
    "    import scipy.io\n",
    "        filename = 'workspace.mat'\n",
    "        mat = scipy.io.loadmat(filename)\n",
    "------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Relational Database: SQL Query\n",
    "    from sqlalchemy import create_engine\n",
    "            engine = create_engine('sqlite:///Northwind.sqlite') \n",
    "            con = engine.connect()\n",
    "            rs = con.execute('SELECT * FROM Orders')\n",
    "            df = pd.DataFrame(rs.fetchall())\n",
    "            df.columns = rs.keys()\n",
    "            con.close()\n",
    "\n",
    "# Using context manager: \n",
    "    engine = create_engine('sqlite:///Northwind.sqlite') \n",
    "        with engine.connect() as con:\n",
    "            rs = con.execute('SELECT * FROM Orders')\n",
    "            df = pd.DataFrame(rs.fetchall(size = 5))                 (here, means first five rows)\n",
    "            df.columns = rs.keys()\n",
    "\n",
    "    engine = create_engine('sqlite:///Northwind.sqlite') \n",
    "            df = pd.read_sql_query('SELECT * FROM Orders', engine)\n",
    "------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Pulling Data From Web:\n",
    "\n",
    "# URL automate file download: \n",
    "    from urllib.request import urlretrieve\n",
    "         url = ' '\n",
    "         urlretrieve(url, ' ')\n",
    "\n",
    "# GET reuquest using urllib: \n",
    "    from urllib.request import urlopen, Request\n",
    "         url = ' '\n",
    "         request = Request(url)\n",
    "         response = urlopen(request)\n",
    "         html = response.read()\n",
    "         response.close()\n",
    "\n",
    "# GET request using requests: \n",
    "    import requests\n",
    "         url = ' '\n",
    "         r = requests.get(url)\n",
    "         text = r.text\n",
    "\n",
    "# BeautifulSoup: extract structured data from HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "         url = ' '\n",
    "         r = requests.get(url)\n",
    "         html_doc = r.text\n",
    "         soup = BeautifulSoup(html_doc)\n",
    "         print(soup.prettify())/print(soup.title)/print(soup.get_text())\n",
    "\n",
    "# JSONs: human readable, dictionary\n",
    "     import json\n",
    "         with open('snakes.json ', 'r') as json_file:\n",
    "              json_data = json.load(json_file)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "**Example**\n",
    "## 1. Using request\n",
    "# Method 1 \n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n",
    "\n",
    "# Method 2\n",
    "from urllib.request import urlopen, Request\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "print(type(response))\n",
    "response.close()\n",
    "\n",
    "## 2. Using BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "pretty_soup = soup.prettify()\n",
    "print(pretty_soup)\n",
    "\n",
    "## 3. Using BeautifulSoup to get text\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "guido_title = soup.title\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "print(guido_text)\n",
    "\n",
    "**API**\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n",
    "\n",
    "**Twitter**\n",
    "# Import package\n",
    "import tweepy, json\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions ##\n",
    "\n",
    "# Define count_entries()\n",
    "def count_entries(df, col_name='lang'):\n",
    "    \"\"\"Return a dictionary with counts of\n",
    "    occurrences as value for each key.\"\"\"\n",
    "    \n",
    "    # Raise a ValueError if col_name is NOT in DataFrame\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError ('The DataFrame does not have a ' + col_name + ' column.')\n",
    "\n",
    "    cols_count = {}   \n",
    "    col = df[col_name]\n",
    "    \n",
    "    for entry in col:\n",
    "        if entry in cols_count.keys():\n",
    "            cols_count[entry] += 1\n",
    "        else:\n",
    "            cols_count[entry] = 1\n",
    "       \n",
    "    return cols_count\n",
    "\n",
    "# Call count_entries(): result1\n",
    "result1 = count_entries(tweets_df, 'lang')\n",
    "\n",
    "# Print result1\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Science Toolbox ##\n",
    "\n",
    "def square(value1, value2):\n",
    "    new_value1 = value1 * value2\n",
    "    new_value2 = value2 * value1\n",
    "    new_tuple = (new_value1, new_value2)\n",
    "    return new_tuple\n",
    "\n",
    "result = suqare(2, 3)\n",
    "print(result)\n",
    "\n",
    "• Tuples: like a list, immutable\n",
    "  Unpack tuples: even_nums = (2, 4, 6)\n",
    "                 a, b, c = even_nums\n",
    "\n",
    "• Local scope - dened inside a function\n",
    "         Global scope - dened in the main body of a script\n",
    "         Built-in scope - names in the pre-dened built-ins module \n",
    "\n",
    "Scopes Searched: Local scop ➡️Enclosing functions➡️Global scope➡️Built-in scope\n",
    "(if a variable is called, mostly it will return the global scope)\n",
    "\n",
    "*args: allows for flexible arugments\n",
    "**kwargs: allows the functions to be returned as a dictionary\n",
    "Default arguments: set in function header\n",
    "\n",
    "Lambda Functions: result = lambda x, y: x**y\n",
    "Map: map applies the function to ALL elements in the sequence \n",
    "          map(func, seq)\n",
    "\n",
    "Error Handling: use try-except\n",
    "\n",
    "Unlike ints and floats, the + operator concatenates strings together, while the *concatenates multiple copies of a string together.\n",
    "len() function is able to handle input arguments such as strings, lists, and tuples, but not int type ones.\n",
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "• Iterator = iter(iterable)\n",
    "i) iterating at once with print(*iterator)\n",
    "ii) After using next on iterable object, the value that has been returned is no longer there\n",
    "iii) It will return error as there is no more vlaue to iterate \n",
    "iv) For iterating over dictionary: for key, value in df.items( ):\n",
    "                                       print(key, value)\n",
    "\n",
    "• Enumerate: for index, value in enumerate(listname, start = 1): \n",
    "                 print(index, value)\n",
    "\n",
    "• Zip: z = zip(listname1, listname2)\n",
    "       for z1, z2 in zip(listname1, listname2): \n",
    "           print(z1, z2)\n",
    "\n",
    "Unpack z: by using * and zip. result1, result2 = zip(*z), then result1 = filename1, result2 = filename2\n",
    "\n",
    "An iterable is an object that can return an iterator, while an iterator is an object that keeps state and produces the next value when you call next() on it.\n",
    "Enumerate() returns an enumerate object that produces a sequence of tuples, and each of the tuples is an index-value pair.\n",
    "Zip: * unpacks an iterable such as a list or a tuple into positional arguments in a function call. You will use * in a call to zip() to unpack the tuples produced by zip().\n",
    "\n",
    "• List Comprehensions: return a list. Use [ ]\n",
    "General: [[output expression] for iterator variable in iterable]\n",
    "Conditional on iterable: [output expression for iterator variable in iterable if predicate expression ]\n",
    "Conditional on ouput expression: [output expression if predicate expression else 0 for iterator variable in iterable]\n",
    "               \n",
    "Dictionary comprehensions: Use curly braces {} instead of brackets [] \n",
    "{key: value for iterator variable in iterable}\n",
    "\n",
    "• Generator: return a generator object. Use ( ) \n",
    "Have to use for loop to print values from generator\n",
    "\n",
    "Result = ([output expression] for iterator variable in iterable)\n",
    "for output expression in result:\n",
    "       print (output expression)\n",
    "\n",
    "• Generator Functions: using def\n",
    "One way to create a generator is to use comprehension syntax. The other way is Generator functions.  \n",
    "Generator functions are functions that yield a series of values. It uses the keyword yield instead of return.\n",
    "Assign name and then use next( ), a function itself is a generator sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Data Science Toolbox Example ## \n",
    "\n",
    "# Iterating over iterables\n",
    "flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']\n",
    "\n",
    "# Print each list item in flash using a for loop\n",
    "for person in flash:\n",
    "    print(person)\n",
    "\n",
    "# Create an iterator for flash: superhero\n",
    "superhero = iter(flash)\n",
    "\n",
    "# Print each item from the iterator\n",
    "print(next(superhero))\n",
    "print(next(superhero))\n",
    "print(next(superhero))\n",
    "print(next(superhero))\n",
    "\n",
    "small_value = iter(range(3))\n",
    "\n",
    "# Print the values in small_value\n",
    "print(next(small_value))\n",
    "print(next(small_value))\n",
    "print(next(small_value))\n",
    "\n",
    "# Loop over range(3) and print the values\n",
    "for num in range(3):\n",
    "    print(num)\n",
    "\n",
    "# Create an iterator for range(10 ** 100): googol\n",
    "googol = iter(range(10 ** 100))\n",
    "\n",
    "# Print the first 5 values from googol\n",
    "print(next(googol))\n",
    "print(next(googol))\n",
    "print(next(googol))\n",
    "print(next(googol))\n",
    "print(next(googol))\n",
    "\n",
    "# Iterators as function arguments\n",
    "values = range(10,21)\n",
    "values_list = list(values)\n",
    "values_sum = sum(range(10,21))\n",
    "\n",
    "print(values_sum)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "# Using enumerate\n",
    "mutants = ['charles xavier', \n",
    "            'bobby drake', \n",
    "            'kurt wagner', \n",
    "            'max eisenhardt', \n",
    "            'kitty pryde']\n",
    "\n",
    "# Create a list of tuples: mutant_list\n",
    "mutant_list = list(enumerate(mutants))\n",
    "\n",
    "# Print the list of tuples\n",
    "print(mutant_list)\n",
    "\n",
    "# Unpack and print the tuple pairs\n",
    "for index1, value1 in enumerate(mutants):\n",
    "    print(index1, value1)\n",
    "\n",
    "# Change the start index\n",
    "for index2, value2 in enumerate(mutants, start = 1):\n",
    "    print(index2, value2)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "# Using zip\n",
    "\n",
    "# Create a list of tuples: mutant_data\n",
    "mutant_data = list(zip(mutants, aliases, powers))\n",
    "\n",
    "# Print the list of tuples\n",
    "print(mutant_data)\n",
    "\n",
    "# Create a zip object using the three lists: mutant_zip\n",
    "mutant_zip = zip(mutants, aliases, powers)\n",
    "\n",
    "# Print the zip object\n",
    "print(mutant_zip)\n",
    "\n",
    "# Unpack the zip object and print the tuple values\n",
    "for value1, value2, value3 in mutant_zip:\n",
    "    print(value1, value2, value3)\n",
    "\n",
    "# Create a zip object from mutants and powers: z1\n",
    "z1 = zip(mutants, powers)\n",
    "\n",
    "# Print the tuples in z1 by unpacking with *\n",
    "print(*z1)\n",
    "\n",
    "# Re-create a zip object from mutants and powers: z1\n",
    "z1 = zip(mutants, powers)\n",
    "\n",
    "# 'Unzip' the tuples in z1 by unpacking with * and zip(): result1, result2\n",
    "result1, result2 = zip(*z1)\n",
    "\n",
    "# Check if unpacked tuples are equivalent to original tuples\n",
    "print(result1 == mutants)\n",
    "print(result2 == powers)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "# Using conditionals in comprehensions\n",
    "\n",
    "fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n",
    "\n",
    "# Create list comprehension: new_fellowship\n",
    "new_fellowship = [member for member in fellowship if len(member) >= 7]\n",
    "print(new_fellowship)\n",
    "\n",
    "# Create list comprehension: new_fellowship1\n",
    "new_fellowship1 = [member if len(member) >= 7 else '' for member in fellowship]\n",
    "print(new_fellowship)\n",
    "\n",
    "# Create dict comprehension: new_fellowship\n",
    "new_fellowship = {member:len(member) for member in fellowship}\n",
    "print(new_fellowship)\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "## Case Study ## \n",
    "\n",
    "zipped_lists = zip(feature_names, row_vals)\n",
    "rs_dict = dict(zipped_lists)\n",
    "\n",
    "# Define lists2dict()\n",
    "def lists2dict(list1, list2):\n",
    "    \"\"\"Return a dictionary where list1 provides\n",
    "    the keys and list2 provides the values.\"\"\"\n",
    "\n",
    "    zipped_lists = zip(list1, list2)\n",
    "    rs_dict = dict(zipped_lists)\n",
    "    return rs_dict\n",
    "\n",
    "rs_fxn = lists2dict(feature_names, row_vals)\n",
    "print(rs_fxn)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Turn list of lists into list of dicts: list_of_dicts\n",
    "list_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]\n",
    "# Turn list of dicts into a DataFrame: df\n",
    "df = pd.DataFrame(list_of_dicts)\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Writing a generator to load data in chunks\n",
    "def read_large_file(file_object):\n",
    "    while True:\n",
    "        data = file_object.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "        \n",
    "with open('world_dev_ind.csv') as file:\n",
    "    gen_file = read_large_file(file)\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "    print(next(gen_file))\n",
    "        \n",
    "counts_dict = {}\n",
    "with open('world_dev_ind.csv') as file:\n",
    "    for line in read_large_file(file):\n",
    "        row = line.split(',')\n",
    "        first_col = row[0]\n",
    "        if first_col in counts_dict.keys():\n",
    "            counts_dict[first_col] += 1\n",
    "        else:\n",
    "            counts_dict[first_col] = 1        \n",
    "print(counts_dict)\n",
    "\n",
    "# Writing an iterator to load data in chunks.1 \n",
    "urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize = 1000)\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "print(df_urb_pop.head())\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "\n",
    "pops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])\n",
    "pops_list = list(pops)\n",
    "print(pops_list)\n",
    "\n",
    "urb_pop_reader = pd.read_csv('ind_pop_data.csv', chunksize=1000)\n",
    "df_urb_pop = next(urb_pop_reader)\n",
    "df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n",
    "pops = zip(df_pop_ceb['Total Population'], \n",
    "           df_pop_ceb['Urban population (% of total)'])\n",
    "pops_list = list(pops)\n",
    "\n",
    "# Writing an iterator to load data in chunks.2 \n",
    "def plot_pop(filename, country_code):\n",
    "\n",
    "    urb_pop_reader = pd.read_csv(filename, chunksize=1000)\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for df_urb_pop in urb_pop_reader:\n",
    "        \n",
    "        df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == country_code]\n",
    "        pops = zip(df_pop_ceb['Total Population'],\n",
    "                    df_pop_ceb['Urban population (% of total)'])\n",
    "        pops_list = list(pops)\n",
    "        df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n",
    "        data = data.append(df_pop_ceb)\n",
    "\n",
    "data.plot(kind='scatter', x='Year', y='Total Urban Population')\n",
    "plt.show()\n",
    "\n",
    "# Set the filename: fn\n",
    "fn = 'ind_pop_data.csv'\n",
    "\n",
    "# Call plot_pop for country code 'CEB'\n",
    "plot_pop(fn, 'CEB')\n",
    "\n",
    "# Call plot_pop for country code 'ARB'\n",
    "plot_pop(fn, 'ARB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
